# FINAL PROJECT (TEAM 17)
## Project: MimicArm

* Team Name: **Code Cartel**
* Team Members: **Surya Sandeep Akella, Venkata Mahati Gollapudi**
* Github Repository URL: https://github.com/upenn-embedded/final-project-code-cartel
* Github Pages Website URL: [for final submission]

## System Block Diagram

![alt text](image-6.png)

### **1. Video**

https://drive.google.com/file/d/1xXghiWyomHCL5a-zHVZRfuj0blY-f9e_/view?usp=sharing

### **2. Images**

![alt text](<WhatsApp Image 2024-11-14 at 22.03.01_b9d1a897.jpg>)

![alt text](<WhatsApp Image 2024-11-22 at 12.06.39_59d96640.jpg>)

![text](<WhatsApp Image 2024-11-22 at 12.06.39_e4016a46.jpg>)

![alt text](<WhatsApp Image 2024-12-02 at 05.26.47_d9d89ff9.jpg>)

![alt text](<WhatsApp Image 2024-12-06 at 03.23.05_f229ed10.jpg>)

<video controls src="WhatsApp Video 2024-11-14 at 22.03.03_528e18b5.mp4" title="Title"></video>


### **3. Results**

The Gesture-Controlled Robotic Arm successfully translated natural hand gestures into precise robotic movements using sensors and servo motors. The MPU6050 IMU detected 3-axis angular velocity for up/down and left/right movements, while a flex sensor controlled the claw's open/close action. Four motors were controlled via PWM signals generated by the ATmega328PB microcontroller. The system achieved real-time synchronization of sensor data and motor control with minimal latency. Robust hardware connections and an external power supply ensured stable motor performance. The robotic arm met all hardware and software specifications, demonstrating smooth and accurate operation during testing. This project provides a functional prototype for applications in robotics, with potential for future enhancements like wireless control or advanced gesture recognition.

#### **3.1 Software Requirements Specification (SRS) Results**

This project is a gesture-controlled robotic arm designed to perform specific movements based on hand gestures. The system uses an IMU (Inertial Measurement Unit) for gyroscopic data and a flex sensor for detecting finger movements. The motors in the robotic arm respond dynamically to these inputs, enabling precise control of movements such as up/down, forward/backward, and left/right. The software integrates real-time sensor data processing and motor control using PWM signals.

**3.1.1  Users**

The primary users of the system include:

1. Robotics Enthusiasts: Hobbyists or students who want to learn about gesture-controlled systems.

2. Industrial Workers: For use in environments where direct manual operation is unsafe.

3. Educational Institutions: For teaching purposes in robotics, mechatronics, or control systems labs.

**3.1.2 Definitions, Abbreviations**

* IMU: Inertial Measurement Unit, used for measuring angular velocity and acceleration.

* PWM: Pulse Width Modulation, used for controlling the servo motors.

* ADC: Analog to Digital Converter, used for reading flex sensor values.

* Gyro: Gyroscope, a component of the IMU that measures angular velocity.

**3.1.3 Functionality**

1. SRS 01: The IMU 3-axis angular velocity will be measured with 16-bit depth every 100 milliseconds +/-10 milliseconds.

2. SRS 02: The flex sensor data will be sampled at a resolution of 10 bits and mapped to a servo motor control angle.

3. SRS 03: The PWM signals will be generated for motor control with a frequency of 50Hz and a duty cycle range corresponding to angles from 30° to 180°.

4. SRS 04: The robotic arm will respond to gestures with a delay of less than 100 milliseconds to ensure real-time operation.

5. SRS 05: The software will clamp motor angles to avoid mechanical damage and keep within predefined limits.

#### **3.2 Hardware Requirements Specification (HRS) Results**

**3.2.1 Definitions, Abbreviations**

* ATmega328P: The microcontroller used as the central processing unit for the system.

* IMU: Inertial Measurement Unit, used for measuring angular velocity.

* PWM: Pulse Width Modulation, used for generating servo control signals.

* ADC: Analog to Digital Converter, used to interpret analog signals from the flex sensor.

* Servo Motor: Actuators used to drive robotic arm movements.

* I2C: Inter-Integrated Circuit, a communication protocol for interfacing sensors or displays.

**3.2.2 Functionality**

1. HRS 01: The project shall be based on the ATmega328P microcontroller, operating at a frequency of 16MHz.

2. HRS 02: An MPU6050 IMU sensor shall be used for detecting 3-axis angular velocity and motion, with a sampling interval of 100 milliseconds +/-10 milliseconds.

3. HRS 03: A flex sensor shall be used to detect finger bending with an angular resolution mapped to servo motor positions.

4. HRS 04: Servo motors will respond to PWM signals with angles ranging from 30° to 180° for precise robotic arm articulation.

5. HRS 05: Power to the system will be provided through a 5V power supply for the microcontroller and peripherals, and a separate power source for the servo motors to prevent voltage drops.

6. HRS 06: The system will include header pins or connectors for external components like sensors and motors to ensure modularity and ease of assembly.

### **4. Conclusion**

The Gesture-Controlled Robotic Arm project successfully demonstrates the integration of sensors, microcontrollers, and servo motors to create an intuitive and interactive system. By leveraging the MPU6050 IMU sensor and a flex sensor, the robotic arm translates natural hand gestures into precise motor movements, enabling control across multiple axes. The system is built around the reliable ATmega328PB microcontroller, ensuring efficient processing of sensor data and generation of PWM signals for servo control.

This project showcases the potential for gesture-based interfaces in applications such as assistive technologies, industrial automation, and remote operations. The design adheres to the specified hardware and software requirements, including accurate timing for IMU data sampling and smooth motion control through PWM signals. While the system performs reliably, potential future enhancements include optimizing sensor calibration, adding more degrees of freedom, and incorporating wireless communication for remote operation.

Overall, the project demonstrates a robust implementation of a gesture-controlled system and highlights its feasibility for practical use cases.

## MVP Demo

* **System Block Diagram (modified as of now)**

![alt text](image-3.png)

### **Current state of the project**

The main aim of this project is to create a gesture controlled robotic arm. To facilitate the purpose, ATMega328PB MCU is used as the main interface for the communication between the arm and the sensors. We assembled the robotic arm along with the four servo motors, one on the base of the arm, two on either side of the arm to control the main motion and one for the claws. For gesture recognition, MPU6050 is used to capture and mimic the hand movements. In addition, a flex sensor is also used. Both of these sensors are fixed on the glove and are interfaced with the MCU. Currently, all the components are wired while our final goal is to wirelessly control the arm based on the gestures. We are able to move the robotic arm based on the movement of the MPU6050 and we have set the initial angles of the servo motors.

### **Hardware Implementation**

1. Robotic arm is assembled along with the servo motors fixed onto the arm
2. MPU6050 is to be fixed onto the glove
3. All the sensors and servo motors are interfaced via the ATMEGA328PB MCU
4. Currently, the arm is controlled by wired connections but we aim to achieve wireless gesture control
5. Voice command module integration is an extension that will be added based on the timeline.       

### **Firmware Implementation**

1. Timer1 Initialization

Purpose: 
Configure Timer1 to generate PWM signals for servo control.

Details:
Phase Correct PWM mode is used for precise control.
ICR1 is set to 39999 for a 20ms PWM period (50Hz frequency).
OC1A (PB1) and OC1B (PB2) are set as PWM output pins.

2. MPU6050 Initialization

Purpose: 
Initialize the MPU6050 gyroscope to wake it up and prepare for data reading.

Details:
I2C communication is initialized using the twi library.
The MPU6050 is set to normal operation mode by writing 0x00 to the power management register (0x6B).

3. Reading Gyroscope Data

Purpose: 
Retrieve the X-axis angular velocity from the MPU6050.

Steps:
The ReadGyroX() function reads the GYRO_XOUT_H register.
Two bytes (high and low) are combined to form a 16-bit value representing angular velocity.
The value is scaled and used to update servo angles.

4. Mapping Gyroscope Data to Servo Angles

Purpose: 
Translate gyroscope readings into servo motor angles.

Details:
A threshold is applied to ignore minor variations (e.g., values between -500 and 500 are ignored).
Gyroscope data is scaled down by dividing by 1000 to reduce sensitivity and ensure smooth transitions.
Servo angles (angleA and angleB) are clamped to valid ranges:
Motor 1: 90° to 150°.
Motor 2: 150° to 180°.

5. Servo Angle Control

Purpose: 
Update the servo motors' positions based on calculated angles.

Details:
The SetServoAngleA() and SetServoAngleB() functions map angles (0° to 180°) to PWM pulse widths (400 to 2600).
This mapping ensures proper servo motion within physical limits.

6. Continuous Update Loop

Purpose: 
Continuously adjust servo angles based on gyroscope readings.

Details:
In the while(1) loop:
Gyroscope data is read periodically.
Servo angles are updated dynamically.
A delay (_delay_ms(200)) smoothens motion by limiting update frequency.

Note: 

⇒ Since the gyro is not soldered and attached to the glove these readings are just for the sake of functionality validation. New values and thresholds will be updated accordingly. 


### **Hardware and Software Specifications and Outcomes**

![alt text](image-5.png)

_Outcomes:_

$i.$ Coding in bare metal for servo raised issues with setting PWM for the servo motors and was finally successful.

$ii.$ MPU6050 anf flex sensor testing was successful but calibration of the values is required.

$iii.$ More refining of the code is required for perfect control of the servo motors. 

$iv.$ Control of the arm using bluetooth was tried but it was not successful due to which we are presenting the arm at the moment with physical connections.

Below are a few proofs attached for the hardware and software outcomes:

![alt text](<WhatsApp Image 2024-11-14 at 22.03.01_b9d1a897.jpg>)

![alt text](<WhatsApp Image 2024-11-22 at 12.06.39_59d96640.jpg>)

### **Latest prototype picture**

![alt text](<WhatsApp Image 2024-12-02 at 05.26.47_d9d89ff9.jpg>)

**Remaining work**

* Coding in bare metal for servo raised issues with setting PWM for the servo motors but finally it was successful.

* MPU6050 and flex sensor testing was successful but calibration of the values is required.

* Control of the arm using bluetooth was tried but it was not successful due to which we are presenting the arm at the moment with physical connections.


**Demo videos**

Working of the servo motors with initial angles set:

https://drive.google.com/file/d/1vt03WgqZA3GA8AHN1IzqQpO4WoGZUQPt/view?usp=sharing

Working of servo integrated with MPU6050:

https://drive.google.com/file/d/1vwsOEEeG3jBWDAu5Wpd28Acv72IgNxB_/view?usp=sharing

### **Riskiest part of the project remaining:** 

Calibrating the IMU and flex sensors to control all 4 motors.

Establishing the wireless connection with the bluetooth module.

### **Plan of action to tackle these risks:**

Fixating the position of the IMUs and soldering the connecting points on the glove so there won't be any margin of error because of improper connections and placement of the sensor.

Explore alternate wireless communication methods or use a traditional wired communication. 



## Sprint review #2

### Current state of project

* Successfully coded and tested the MPU6050 with ATmega328PB in bare metal 
* Successfully controlled servo motors using ATmega328PB in bare metal
* Successfully coded and tested flex sensor in bare metal

_Testing the MPU6050_
![alt text](<WhatsApp Image 2024-11-22 at 12.06.39_59d96640.jpg>)
![alt text](<WhatsApp Image 2024-11-22 at 12.06.39_e4016a46.jpg>)

_Testing the servo motors using bare metal C: all the 4 motors were controlled in the similar fashion_

https://drive.google.com/file/d/1mKeSxIoLztbpDnmUFKWJuMLxZQF2OpbM/view?usp=sharing

### Last week's progress 

* Received the components except flex sensors and verified them
* Assembled the robotic arm
* Tested the robotic arm working using basic arduino code to check for any hardware issue; none found but the assembly needs to be made more firm and precise. 
* Soldered the required components
* Tried to test basic communication between two ESP32s 

### Next week's plan

* Making the glove
* Establishing communication between glove and arm
* Test the communication using bluetooth module

## Sprint review #1

### Current state of project

* Received the components except flex sensors and verified them
* Assembled the robotic arm
* Tested the robotic arm working using basic arduino code to check for any hardware issue; none found but the assembly needs to be made more firm and precise. 
* Soldered the required components
* Tried to test basic communication between two ESP32s 

_Video and images are added for reference:_

![alt text](<WhatsApp Image 2024-11-14 at 22.03.01_b9d1a897.jpg>)

Link to video: 

https://drive.google.com/file/d/1Ogfg4GHIjfzqlwnis5ji1rMkxbys6zJ2/view?usp=sharing

### Last week's progress 

Finalized the components and took opinions from TAs about our project and clarified our doubts. Projected the workflow to be executed.

### Next week's plan

* Start working on the servo motor code in bare metal and analyze the working for different angles of motion of the arm.
* WiFi connectivity
* Understand and try to start implementing gesture contol mechanism.  

## Sprint review trial

### Current state of project
### Last week's progress 
### Next week's plan
_____
## Final Project Proposal

### 1. Abstract

The MimicArm is a system that enables users to control a robotic arm using hand gestures. The arm processes data  to translate hand movements into precise actions. Integration with the Blynk IoT platform allows remote control via a smartphone app, providing real-time monitoring and flexibility. 

Additionally, a voice control feature can enhance accessibility by enabling users to operate the arm through simple voice commands (time constrained). This versatile system is ideal for applications in remote operation, industrial automation, and assistive technology.


### 2. Motivation

* This project is about bringing human-like interaction to technology, making it smarter, more accessible, and adaptable to various needs.

* Imagine being able to control a robotic arm by just moving your hand or saying a few words.

* It enhances possibilities for remote work, assists individuals with limited mobility, and simplifies tasks in everyday life. 

* The added IoT connectivity lets users control the arm from anywhere, making it versatile for applications ranging from industrial settings to home use. 


### 3. Goals

* Operation using natural hand gestures for  more user-friendly experience.

* Integrate IoT connectivity via the Blynk platform for real-time monitoring and control from anywhere.

* Utilize affordable components to create a budget-friendly solution for wider accessibility.

* Develop a setup that can be easily customized and upgraded for future needs.

* Voice command functionality may also be included to provide an additional control option, depending on project feasibility.


### 4. System Block Diagram

![alt text](<WhatsApp Image 2024-10-28 at 00.03.48_cd97ffd3.jpg>)

### 5. Design Sketches

![alt text](image.png)

### 6. Hardware and Software Requirements

![alt text](image-1.png)

### 7. Methodology

    1. System Design

    Arm Assembly: Start by assembling the robotic arm, ensuring that all components, including servos, joints, and frames, are securely connected and properly aligned.
    Motor Testing: Perform initial testing of the servo motors to verify they respond correctly to PWM signals and have sufficient torque for the required movements.
    Power Management: Design and implement a power management system to ensure consistent and stable power delivery to all components, including the microcontrollers, servos, and sensors.
    
    2.Determining Gesture

    IMU Setup and Processing Data: Integrate an IMU sensor on the hand controller, and configure it to accurately capture motion data. Develop algorithms to process accelerometer and gyroscope readings.
    Determining Valid Gestures Based on Processed Information: Translate the processed IMU data into recognizable gestures. Define and program specific gestures that will correspond to particular movements of the robotic arm.

    3. Data Transmission

    Setup ESPs for Communication: Configure two ESP Wi-Fi modules—one for the hand controller and the other for the robotic arm controller—to establish a reliable wireless communication link.
    Testing Data Transfer and Control Based on Transferred Data: Conduct tests to verify that the data is transmitted accurately between the hand and arm controllers. Ensure that the robotic arm responds correctly to the gestures detected on the hand controller.

    4. IoT Integration

    Setup Blynk: Use the Blynk platform to create a user-friendly interface for remote control of the robotic arm, allowing users to send commands and monitor status via a smartphone app.
    Connecting ESP to Wi-Fi and Blynk: Configure the ESP module to connect to a local Wi-Fi network and sync with the Blynk cloud server to facilitate seamless remote operation.
    Control via Blynk: Implement the capability for users to control the robotic arm remotely using the Blynk app, providing real-time adjustments and monitoring.

    5. Voice Command Control

    Integrating Voice Command Feature as per Feasibility: Depending on project scope and feasibility, integrate voice command functionality, allowing users to issue simple verbal instructions to control the robotic arm, complementing gesture and IoT controls. This will involve setting up a voice recognition module connected to the ATmega328PB for processing voice inputs.


### 8. Evaluation and Final Demo

    1. Ability to accurately control the arm based on the hand gestures

    2. Integration with Blynk and controlling the arm remotely 

    3. Testing the functionality of the arm with some real-time use cases

    4. Voice commands, if included, should successfully control the arm


### 12. Proposal Presentation

Add your slides to the Final Project Proposal slide deck in the Google Drive.

Team 17: MIMIC ARM 
https://docs.google.com/presentation/d/1Jt2f54SHfv9yaWAEWJdul8VxFSMtWL5YvAFFXDAgIP4/edit#slide=id.g30f18db8311_186_0

## References

Fill in your references here as you work on your proposal and final submission. Describe any libraries used here.

